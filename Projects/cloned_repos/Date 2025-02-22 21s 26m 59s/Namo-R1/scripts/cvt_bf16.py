import torch
from transformers import AutoModel, AutoTokenizer
import argparse
import os


def convert_and_save_bf16(model_path, output_dir=None):

    try:
        if output_dir is None:
            output_dir = model_path.strip("/") + "_bf16"
        os.makedirs(output_dir, exist_ok=True)
        print(f"â³ æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹æ¥è‡ª: {model_path}")

        model = AutoModel.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,  # åˆå§‹åŠ è½½ä¸ºBF16
            # device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
            low_cpu_mem_usage=True,  # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            trust_remote_code=True,
        )

        print("ğŸ”§ æ­£åœ¨è½¬æ¢æ¨¡å‹æƒé‡åˆ°BF16...")
        model = model.to(torch.bfloat16)

        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜BF16æ¨¡å‹åˆ°: {output_dir}")
        model.save_pretrained(
            output_dir,
            safe_serialization=True,  # ä½¿ç”¨safetensorsæ ¼å¼
            max_shard_size="6GB",  # åˆ†ç‰‡å¤§å°
        )

        # ä¿å­˜tokenizer
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path, trust_remote_code=True
            )
            tokenizer.save_pretrained(output_dir)
        except Exception as e:
            print("passing save tokenzier.")

        print("âœ… è½¬æ¢å®Œæˆï¼ä¿å­˜å†…å®¹ï¼š")
        print(f"   - æ¨¡å‹æƒé‡: {output_dir}/pytorch_model*.bin")
        print(f"   - é…ç½®æ–‡ä»¶: {output_dir}/config.json")
        print(f"   - Tokenizeræ–‡ä»¶: {output_dir}/tokenizer.*")

    except Exception as e:
        print(f"âŒ é”™è¯¯å‘ç”Ÿ: {str(e)}")
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è½¬æ¢HFæ¨¡å‹åˆ°BF16æ ¼å¼")
    parser.add_argument(
        "model_path",
        type=str,
        help="è¾“å…¥æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°ç›®å½•æˆ–HF Hubåç§°ï¼‰",
    )
    parser.add_argument("--output_dir")

    args = parser.parse_args()

    convert_and_save_bf16(args.model_path, args.output_dir)
